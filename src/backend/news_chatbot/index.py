"""
Simple News Chatbot Handler

AWS Lambda Powertools ì˜ì¡´ì„± ì—†ì´ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©í•˜ëŠ” ë²„ì „ì…ë‹ˆë‹¤.
"""

import json
import logging
import os
import re
from datetime import datetime
from typing import Any, Dict, Optional, List, Tuple
from difflib import SequenceMatcher
from urllib.parse import urlparse

import boto3
from botocore.exceptions import ClientError
import requests

# Perplexity API settings
PERPLEXITY_API_KEY = os.environ.get("PERPLEXITY_API_KEY")
PPLX_URL = "https://api.perplexity.ai/chat/completions"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
KNOWLEDGE_BASE_ID = os.environ.get("KNOWLEDGE_BASE_ID")
LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(getattr(logging, LOG_LEVEL))

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
bedrock_runtime = boto3.client("bedrock-runtime")
bedrock_agent_runtime = boto3.client("bedrock-agent-runtime")
s3_client = boto3.client("s3")


class ChatbotError(Exception):
    """ì±—ë´‡ ê´€ë ¨ ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸"""
    pass


def expand_query_with_ai(original_query: str) -> str:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì§ˆë¬¸ì„ í™•ì¥í•©ë‹ˆë‹¤."""
    try:
        current_year = datetime.now().year
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë‰´ìŠ¤ ê²€ìƒ‰ì— ë” ì í•©í•˜ë„ë¡ í™•ì¥í•´ì£¼ì„¸ìš”. 
ì›ë³¸ ì§ˆë¬¸: "{original_query}"

í™•ì¥ ê·œì¹™:
1. ê¸°ì—…ëª…ì´ ìˆìœ¼ë©´ ê´€ë ¨ ê³„ì—´ì‚¬, ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ ì¶”ê°€
2. ê²½ì œ ìš©ì–´ê°€ ìˆìœ¼ë©´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€  
3. ìµœëŒ€ 3-4ê°œì˜ ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì¶”ê°€
4. í•œêµ­ ê²½ì œ/ê¸°ì—… ë‰´ìŠ¤ ë§¥ë½ì—ì„œ í™•ì¥
5. ì¤‘ìš”: í˜„ì¬ ë…„ë„ëŠ” {current_year}ë…„ì…ë‹ˆë‹¤. ìµœì‹  ì´ìŠˆì¸ ê²½ìš° {current_year}ë…„ ë˜ëŠ” {current_year-1}ë…„ í‚¤ì›Œë“œ ì¶”ê°€

í™•ì¥ëœ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):"""

        response = bedrock_runtime.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 100,
                "messages": [
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ]
            })
        )
        
        result = json.loads(response['body'].read())
        expanded_query = result['content'][0]['text'].strip()
        
        logger.info(f"AI expanded query: '{original_query}' -> '{expanded_query}'")
        return expanded_query
        
    except Exception as e:
        logger.warning(f"Failed to expand query with AI: {str(e)}")
        return original_query


# -----------------------
# Helper: detect if external search needed
# -----------------------

DATE_KEYWORDS = [
    # ì¼(day) ë‹¨ìœ„
    "ê·¸ì œ", "ê·¸ì œì´í‹€ ì „", "ê·¸ì €ê»˜", "ì–´ì œ", "ì–´ì €ê»˜", "ì „ì¼", "ê¸ˆì¼", "ì˜¤ëŠ˜", "ë‹¹ì¼", "ë‚´ì¼", "ìµì¼", "ëª¨ë ˆ", "ê¸€í”¼",
    "5ì¼ ì „", "3ì¼ ì „", "2ì¼ ì „",

    # ì£¼(week) ë‹¨ìœ„
    "ì§€ë‚œì£¼", "ì§€ë‚œ ì£¼", "ì§€ì§€ë‚œì£¼", "ì§€ì§€ë‚œ ì£¼", "ì§€ë‚œ í•œ ì£¼", "ì§€ë‚œ 1ì£¼ì¼", "ê¸ˆì£¼", "ì´ë²ˆ ì£¼", "ì´ë²ˆì£¼",
    "ì´ë²ˆ í•œ ì£¼", "ì´ë²ˆ 1ì£¼ì¼", "ë‹¤ìŒì£¼", "ë‹¤ìŒ ì£¼", "ì°¨ì£¼", "ë‹¤ìŒ í•œ ì£¼", "ë‹¤ìŒ 1ì£¼ì¼",
    "2ì£¼ ì „", "3ì£¼ ì „", "4ì£¼ ì „",

    # ì›”(month) ë‹¨ìœ„
    "ì§€ë‚œë‹¬", "ì§€ë‚œ ë‹¬", "ì§€ë‚œ í•œ ë‹¬", "ì§€ë‚œ 1ê°œì›”", "ì§€ì§€ë‚œë‹¬", "ì§€ì§€ë‚œ ë‹¬",
    "ê¸ˆì›”", "ì´ë²ˆ ë‹¬", "ì´ë²ˆë‹¬", "ì´ë²ˆ í•œ ë‹¬", "ì´ë²ˆ 1ê°œì›”",
    "ë‹¤ìŒë‹¬", "ë‹¤ìŒ ë‹¬", "ì°¨ì›”", "ë‹¤ìŒ í•œ ë‹¬", "ë‹¤ìŒ 1ê°œì›”",
    "2ê°œì›” ì „", "3ê°œì›” ì „", "6ê°œì›” ì „",

    # ë¶„ê¸°(quarter) ë‹¨ìœ„
    "ì§€ë‚œ ë¶„ê¸°", "ì§€ë‚œë¶„ê¸°", "ì´ë²ˆ ë¶„ê¸°", "ì´ë²ˆë¶„ê¸°", "ë‹¤ìŒ ë¶„ê¸°", "ë‹¤ìŒë¶„ê¸°",

    # ë°˜ê¸°(half-year)
    "ìƒë°˜ê¸°", "í•˜ë°˜ê¸°", "ì‘ë…„ ìƒë°˜ê¸°", "ì§€ë‚œ ìƒë°˜ê¸°", "ì‘ë…„ í•˜ë°˜ê¸°", "ì§€ë‚œ í•˜ë°˜ê¸°",

    # ì—°(year) ë‹¨ìœ„
    "ì‘ë…„", "ì§€ë‚œ í•´", "ì§€ë‚œí•´", "ì§€ë‚œ í•œ í•´", "ì§€ë‚œ 1ë…„",
    "ì¬ì‘ë…„", "2ë…„ ì „", "3ë…„ ì „", "5ë…„ ì „", "10ë…„ ì „",
    "ê¸ˆë…„", "ê¸ˆë…„ë„", "ì˜¬í•´", "ì˜¬ í•´", "2025ë…„", "2024ë…„",
    "ë‚´ë…„", "ë‹¤ìŒ í•´", "ë‹¤ìŒí•´", "ì°¨ë…„", "ë‚´ë…„ë„",
    "2ë…„ í›„", "3ë…„ í›„",

    # ëŠìŠ¨í•œ/ë¹„ì •í˜• í‘œí˜„
    "ì–¼ë§ˆ ì „", "ì–¼ë§ˆ ì§€ë‚˜ì§€ ì•Šì•„", "ì¡°ë§Œê°„", "ë¨¸ì§€ì•Šì•„", "ê³§", "ë¹ ë¥¸ ì‹œì¼ ë‚´",
    "ë°”ë¡œ ì „", "ë°”ë¡œ í›„"
]



def needs_external_search(question: str) -> bool:
    """ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë‚ ì§œÂ·ì‹œì‚¬ì„± í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True"""
    lower_q = question.lower()
    if any(k.replace(" ", "") in lower_q for k in [kw.replace(" ", "") for kw in DATE_KEYWORDS]):
        return True
    # ë„ˆë¬´ ëª¨í˜¸í•˜ê±°ë‚˜ ì§§ì€ ì§ˆë¬¸ë„ ì™¸ë¶€ ê²€ìƒ‰(ì˜ˆ: 'ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆì–´?')
    if len(question.strip()) <= 4:
        return True
    return False


# -----------------------
# Helper: typo detection & Perplexity-based spell-fix
# -----------------------

def is_typo(question: str) -> bool:
    """ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: ììŒ/ëª¨ìŒ ë‹¨ë…Â·ì˜ë¬¸ ë‚œë…Â·í¸ì§‘ê±°ë¦¬ë¡œ ì˜¤íƒ€ ì—¬ë¶€ íŒë‹¨"""
    # í•œê¸€ ììŒ/ëª¨ìŒë§Œ 2ê¸€ì ì´ìƒ ì—°ì† â†’ ì˜¤íƒ€ ê°€ëŠ¥ì„±
    if re.search(r"[ã„±-ã…ã…-ã…£]{2,}", question):
        return True
    # ì˜ë¬¸ ì—°ì† 4ì ì´ìƒ(í•œêµ­ì–´ ë§¥ë½ì—ì„œ í”ì¹˜ ì•ŠìŒ)
    if re.search(r"[a-zA-Z]{4,}", question):
        return True
    # ì‚¬ì „ ì£¼ìš” í‚¤ì›Œë“œì™€ í¸ì§‘ê±°ë¦¬ í™•ì¸ (ê°„ë‹¨ ìƒ˜í”Œ)
    vocab = ["ì‚¼ì„±ì „ì", "ê¸ˆë¦¬", "í™˜ìœ¨", "ë¶€ë™ì‚°", "ì£¼ê°€", "ì¸í”Œë ˆì´ì…˜"]
    for w in vocab:
        if SequenceMatcher(None, w, question).ratio() > 0.8:
            return False
    return False  # ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜ (ì‹¬ê° ì˜¤íƒ€ë§Œ True)


def perplexity_spellfix(question: str) -> Tuple[str, str]:
    """Perplexityë¡œ ì˜¤íƒ€ êµì •Â·í‚¤ì›Œë“œ ì¶”ì¶œ"""
    prompt = (
        "ë‹¤ìŒ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì˜¬ë°”ë¥´ê²Œ êµì •í•œ ë’¤ JSON í˜•íƒœë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "í¬ë§·: {\"corrected\":\"...\", \"keywords\":[\"...\"]}\n"
        f"ë¬¸ì¥: \"{question}\""
    )
    resp = query_perplexity(prompt, max_tokens=200)
    try:
        data = json.loads(resp)
        corrected = data.get("corrected", question)
        kws = ", ".join(data.get("keywords", []))
        return corrected, f"ì˜¤íƒ€ êµì • í‚¤ì›Œë“œ: {kws}"
    except Exception as err:
        logger.warning(f"Spellfix JSON parse error: {err} â€“ raw: {resp[:100]}")
        raise ChatbotError("Perplexity spellfix ì‹¤íŒ¨")


# -----------------------
# Helper: hard question refine
# -----------------------

def perplexity_refine(question: str) -> Tuple[str, str]:
    """Perplexityë¡œ ë‚ ì§œÂ·ì‹œì‚¬ì„± ì§ˆë¬¸ ì˜ë„ ë³´ê°•"""
    current_year = datetime.now().year
    current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    
    prompt = (
        f"í˜„ì¬ ë‚ ì§œ: {current_date}\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸ì„ í•œêµ­ ë‰´ìŠ¤ ê²€ìƒ‰ì— ì í•©í•˜ë„ë¡ ì •ì œí•˜ì„¸ìš”.\n"
        "ìƒëŒ€ì  ë‚ ì§œ('ì–´ì œ', 'ì˜¤ëŠ˜', 'ìµœê·¼' ë“±)ëŠ” êµ¬ì²´ì  ë‚ ì§œë‚˜ ë…„ë„ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n"
        f"ìµœì‹  ì´ìŠˆì˜ ê²½ìš° {current_year}ë…„ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
        "JSONë§Œ ë°˜í™˜: {\"refined_query\":\"...\", \"summary\":\"...150ì ë‚´\", \"suggested_years\":[\"2024\", \"2025\"]} \n"
        f"ì§ˆë¬¸: {question}"
    )
    resp = query_perplexity(prompt, max_tokens=300)
    try:
        data = json.loads(resp)
        refined_query = data.get("refined_query", question)
        summary = data.get("summary", "")
        years = data.get("suggested_years", [])
        
        # ì—°ë„ í‚¤ì›Œë“œë¥¼ refined_queryì— ì¶”ê°€
        if years:
            year_keywords = " ".join(years)
            refined_query = f"{refined_query} {year_keywords}"
            
        return refined_query, summary
    except Exception as err:
        logger.warning(f"Refine JSON parse error: {err} â€“ raw: {resp[:100]}")
        raise ChatbotError("Perplexity refine ì‹¤íŒ¨")


def orchestrated_news_search(query: str, max_retries: int = 3) -> Dict[str, Any]:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ - ë‹¨ê³„ë³„ ë¶„ì„ ë° ì¬ì‹œë„ ë¡œì§"""
    
    current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    current_year = datetime.now().year
    
    # Step 1: ì§ˆë¬¸ ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½
    analysis_prompt = f"""í˜„ì¬ ë‚ ì§œ: {current_date}

ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µ:
{{
    "user_goal": "ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ",
    "time_context": "ì§ˆë¬¸ì˜ ì‹œê°„ì  ë§¥ë½ (ì˜ˆ: 2025ë…„ 6ì›”, ìµœê·¼, ê³¼ê±° ë“±)",
    "target_year_range": ["2024", "2025"],
    "key_entities": ["í•µì‹¬ í‚¤ì›Œë“œë“¤"],
    "search_strategy": "ê²€ìƒ‰ ì „ëµ ì„¤ëª…",
    "expected_article_timeframe": "ê¸°ëŒ€í•˜ëŠ” ê¸°ì‚¬ ì‹œê°„ëŒ€"
}}"""

    try:
        analysis_response = bedrock_runtime.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 400,
                "messages": [{"role": "user", "content": analysis_prompt}]
            })
        )
        
        analysis_result = json.loads(analysis_response['body'].read())
        analysis_text = analysis_result['content'][0]['text'].strip()
        
        # JSON ì¶”ì¶œ
        analysis_data = json.loads(analysis_text)
        logger.info(f"Query analysis: {analysis_data}")
        
    except Exception as e:
        logger.warning(f"Analysis failed: {e}")
        analysis_data = {
            "user_goal": query,
            "target_year_range": [str(current_year), str(current_year-1)],
            "key_entities": [query],
            "search_strategy": "basic search"
        }

    # Step 2: ê²€ìƒ‰ ì‹œë„ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
    for attempt in range(max_retries):
        logger.info(f"Search attempt {attempt + 1}/{max_retries}")
        
        # ì‹œë„ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        if attempt == 0:
            # ì²« ì‹œë„: ì—°ë„ + í•µì‹¬ í‚¤ì›Œë“œ
            search_query = f"{' '.join(analysis_data.get('key_entities', [query]))} {' '.join(analysis_data.get('target_year_range', []))}"
        elif attempt == 1:
            # ë‘ ë²ˆì§¸ ì‹œë„: í‚¤ì›Œë“œë§Œ
            search_query = ' '.join(analysis_data.get('key_entities', [query]))
        else:
            # ë§ˆì§€ë§‰ ì‹œë„: ì›ë³¸ ì§ˆë¬¸
            search_query = query
            
        logger.info(f"Attempt {attempt + 1} search query: {search_query}")
        
        # Bedrock ê²€ìƒ‰ ì‹¤í–‰
        search_result = execute_bedrock_search(search_query, analysis_data)
        
        # ê²°ê³¼ í‰ê°€
        if evaluate_search_results(search_result, analysis_data, query):
            logger.info(f"Search succeeded on attempt {attempt + 1}")
            return search_result
        else:
            logger.warning(f"Search attempt {attempt + 1} failed quality check")
    
    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ Perplexity í´ë°±
    logger.warning("All search attempts failed, using Perplexity fallback")
    return perplexity_fallback_search(query)


def execute_bedrock_search(search_query: str, analysis_data: Dict) -> Dict[str, Any]:
    """Bedrock Knowledge Base ê²€ìƒ‰ ì‹¤í–‰"""
    try:
        # ê²€ìƒ‰ ì‹¤í–‰
        retrieve_response = bedrock_agent_runtime.retrieve(
            knowledgeBaseId=KNOWLEDGE_BASE_ID,
            retrievalQuery={"text": search_query},
            retrievalConfiguration={
                "vectorSearchConfiguration": {
                    "numberOfResults": 5,
                    "overrideSearchType": "HYBRID"
                }
            }
        )
        
        retrieval_results = retrieve_response.get('retrievalResults', [])
        
        if not retrieval_results:
            raise ChatbotError("No search results found")
            
        return generate_orchestrated_response(search_query, retrieval_results, analysis_data)
        
    except Exception as e:
        logger.error(f"Bedrock search failed: {e}")
        raise ChatbotError(f"ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


def evaluate_search_results(search_result: Dict, analysis_data: Dict, original_query: str) -> bool:
    """ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€"""
    try:
        sources = search_result.get('sources', [])
        if not sources:
            return False
            
        # ë‚ ì§œ ê´€ë ¨ì„± ì²´í¬
        target_years = analysis_data.get('target_year_range', [])
        if target_years:
            relevant_articles = 0
            for source in sources:
                source_date = source.get('date', '')
                if any(year in source_date for year in target_years):
                    relevant_articles += 1
            
            # ìµœì†Œ 60% ì´ìƒì´ ê´€ë ¨ ë…„ë„ì—¬ì•¼ í•¨ (ê¸°ì¤€ ìƒí–¥)
            relevance_ratio = relevant_articles / len(sources)
            logger.info(f"Date relevance ratio: {relevance_ratio:.2f} for years {target_years}")
            if relevance_ratio < 0.6:
                logger.warning(f"Low date relevance: {relevance_ratio:.2f}")
                return False
        
        return True
        
    except Exception as e:
        logger.error(f"Result evaluation failed: {e}")
        return False


def generate_orchestrated_response(query: str, retrieval_results: List, analysis_data: Dict) -> Dict[str, Any]:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ëœ ì‘ë‹µ ìƒì„±"""
    # ë‚ ì§œ í•„í„°ë§ ì ìš©ëœ ê¸°ì‚¬ ì„ ë³„
    target_years = analysis_data.get('target_year_range', [])
    filtered_results = []
    
    for result in retrieval_results[:10]:  # ë” ë§ì€ ê²°ê³¼ì—ì„œ í•„í„°ë§
        # S3 URIì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œí•˜ì—¬ ë‚ ì§œ í™•ì¸
        s3_location = result.get('location', {}).get('s3Location', {})
        s3_uri = s3_location.get('uri', '')
        
        if s3_uri:
            try:
                content = result.get('content', {}).get('text', '')
                metadata = find_best_matching_article(s3_uri, content)
                article_date = metadata.get("date", "") if metadata else ""
                
                # ë‚ ì§œê°€ íƒ€ê²Ÿ ë…„ë„ì™€ ë§¤ì¹˜ë˜ëŠ”ì§€ í™•ì¸
                if target_years and article_date:
                    date_match = any(year in article_date for year in target_years)
                    if date_match:
                        filtered_results.append(result)
                        logger.info(f"âœ… Orchestration: Included article from {article_date}")
                    else:
                        logger.info(f"ğŸš« Orchestration: Filtered out article from {article_date}")
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í¬í•¨ (ì•ˆì „í•œ ê¸°ë³¸ê°’)
                    filtered_results.append(result)
                    
            except Exception as e:
                logger.warning(f"Error filtering article by date: {e}")
                filtered_results.append(result)  # ì—ëŸ¬ ì‹œ í¬í•¨
        else:
            filtered_results.append(result)  # S3 URIê°€ ì—†ìœ¼ë©´ í¬í•¨
        
        if len(filtered_results) >= 5:  # ìµœëŒ€ 5ê°œë§Œ ì‚¬ìš©
            break
    
    # í•„í„°ë§ í›„ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ê²°ê³¼ ì‚¬ìš©
    if len(filtered_results) < 2 and len(retrieval_results) > 2:
        logger.warning(f"Orchestration: Date filtering left only {len(filtered_results)} articles, using original results")
        filtered_results = retrieval_results[:5]
    
    # ê¸°ì‚¬ë“¤ì„ ë²ˆí˜¸ë¡œ í¬ë§·
    formatted_articles = []
    for i, result in enumerate(filtered_results[:5], 1):
        content = result.get('content', {}).get('text', '')
        formatted_articles.append(f"[ê¸°ì‚¬ {i}]\n{content}")
    
    articles_text = '\n\n'.join(formatted_articles)
    
    # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸
    enhanced_prompt = f"""ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼:
- ì‚¬ìš©ì ëª©í‘œ: {analysis_data.get('user_goal', 'ì •ë³´ ê²€ìƒ‰')}
- ì‹œê°„ì  ë§¥ë½: {analysis_data.get('time_context', 'ì¼ë°˜ì ')}
- í•µì‹¬ ì—”í‹°í‹°: {', '.join(analysis_data.get('key_entities', []))}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles_text}

**ì¤‘ìš” ì§€ì¹¨:**
1. ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ì‹œê°„ì  ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€
2. ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ ê¸°ì‚¬ë§Œ ì„ ë³„í•˜ì—¬ ì¸ìš©
3. ë‚ ì§œê°€ ë§ì§€ ì•ŠëŠ” ê¸°ì‚¬ëŠ” ì œì™¸í•˜ê³  ì„¤ëª…
4. ê°ì£¼ëŠ” [1], [2], [3], [4], [5] ìˆœì„œë¡œë§Œ ì‚¬ìš©

**ê°ì£¼ ê·œì¹™:**
- ì²« ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [1]
- ë‘ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [2]
- ì„¸ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [3]
- ë„¤ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [4]
- ë‹¤ì„¯ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [5]

ë‹µë³€ ì‘ì„±:"""

    # AI ì‘ë‹µ ìƒì„±
    ai_response = bedrock_runtime.invoke_model(
        modelId="anthropic.claude-3-haiku-20240307-v1:0",
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1000,
            "messages": [{"role": "user", "content": enhanced_prompt}]
        })
    )
    
    result = json.loads(ai_response['body'].read())
    answer = result['content'][0]['text'].strip()
    
    # ì‘ë‹µ êµ¬ì¡° ìƒì„±
    response = {
        "output": {"text": answer},
        "citations": [{
            "generatedResponsePart": {
                "textResponsePart": {
                    "span": {"start": 0, "end": len(answer)},
                    "text": answer
                }
            },
            "retrievedReferences": []
        }],
        "sessionId": f"orchestrated-{datetime.now().isoformat()}"
    }
    
    # ë‚ ì§œ í•„í„°ë§ëœ ê²°ê³¼ë§Œ ì°¸ì¡°ë¡œ ì¶”ê°€
    for retrieval_result in filtered_results:
        reference = {
            "content": {"text": retrieval_result.get('content', {}).get('text', '')},
            "location": retrieval_result.get('location', {}),
            "metadata": retrieval_result.get('metadata', {})
        }
        response['citations'][0]["retrievedReferences"].append(reference)
    
    return response


def perplexity_fallback_search(query: str) -> Dict[str, Any]:
    """Perplexityë¥¼ ì‚¬ìš©í•œ í´ë°± ê²€ìƒ‰"""
    try:
        current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
        fallback_prompt = f"""í˜„ì¬ ë‚ ì§œ: {current_date}

"{query}"ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•„ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•˜ê³ , êµ¬ì²´ì ì¸ ë‚ ì§œì™€ ì¶œì²˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”."""

        pplx_response = query_perplexity(fallback_prompt, max_tokens=500)
        
        return {
            "output": {"text": pplx_response},
            "citations": [],
            "sessionId": f"perplexity-{datetime.now().isoformat()}"
        }
        
    except Exception as e:
        logger.error(f"Perplexity fallback failed: {e}")
        raise ChatbotError("ëª¨ë“  ê²€ìƒ‰ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


def retrieve_and_generate_with_references(query: str, max_results: int = 10, extra_context: str = "") -> Dict[str, Any]:
    """Bedrock Knowledge Baseì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. Referencesë„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ í™•ì¥
        expanded_query = expand_query_with_ai(query)
        
        logger.info(f"Querying knowledge base with expanded query: {expanded_query}")
        
        # 1. retrieve APIë¡œ ì •í™•íˆ 5ê°œ ê¸°ì‚¬ ê²€ìƒ‰
        retrieve_response = bedrock_agent_runtime.retrieve(
            knowledgeBaseId=KNOWLEDGE_BASE_ID,
            retrievalQuery={
                "text": expanded_query
            },
            retrievalConfiguration={
                "vectorSearchConfiguration": {
                    "numberOfResults": 5,  # ì •í™•íˆ 5ê°œ
                    "overrideSearchType": "HYBRID"
                }
            }
        )
        
        retrieval_results = retrieve_response.get('retrievalResults', [])
        logger.info(f"Retrieved {len(retrieval_results)} results from retrieve API")
        
        if not retrieval_results:
            raise ChatbotError("ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # 2. ê²€ìƒ‰ëœ ê¸°ì‚¬ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ í¬ë§·
        formatted_articles = []
        for i, result in enumerate(retrieval_results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ ì‚¬ìš©
            content = result.get('content', {}).get('text', '')
            formatted_articles.append(f"[ê¸°ì‚¬ {i}]\n{content}")
        
        articles_text = '\n\n'.join(formatted_articles)
        
        # 3. ì§ì ‘ AI ëª¨ë¸ì— ì§ˆë¬¸ ì „ì†¡ (retrieveAndGenerate ëŒ€ì‹ )

        context_block = f"\n\nì¶”ê°€ ì°¸ê³  ìë£Œ (ì‹¤ì‹œê°„ ê²€ìƒ‰):\n{extra_context}\n" if extra_context else ""

        prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

{context_block}

ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles_text}

**ì‘ì—… ìˆœì„œ:**
1. ê° ê¸°ì‚¬ì˜ ì œëª©, ë‚ ì§œë¥¼ í™•ì¸í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ê²€í† 
2. ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì •ë³´ê°€ ìˆëŠ” ê¸°ì‚¬ë“¤ì„ ì„ ë³„
3. ì„ ë³„ëœ ê¸°ì‚¬ë“¤ì—ì„œ ì¸ìš©í•  ë¬¸ì¥ë“¤ì„ ì¶”ì¶œ
4. ì¶”ì¶œëœ ë¬¸ì¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë‹µë³€ ì‘ì„±
5. ì¸ìš©í•œ ë¬¸ì¥ ë’¤ì— ë°˜ë“œì‹œ ê°ì£¼ ë²ˆí˜¸ ì¶”ê°€

**ê°ì£¼ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):**
- ì²« ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [1]
- ë‘ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [2]
- ì„¸ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [3]
- ë„¤ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [4]
- ë‹¤ì„¯ ë²ˆì§¸ë¡œ ì¸ìš©í•˜ëŠ” ê¸°ì‚¬: [5]
- ë°˜ë“œì‹œ [1]ë¶€í„° ì‹œì‘í•´ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©
- [6], [7], [8] ë“± 6ë²ˆ ì´ìƒì˜ ìˆ«ìëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
- ê°™ì€ ê¸°ì‚¬ë¥¼ ì—¬ëŸ¬ ë²ˆ ì¸ìš©í•  ë•Œë„ ê°™ì€ ë²ˆí˜¸ ì‚¬ìš©

**ë‹µë³€ ì‘ì„± ì§€ì¹¨:**
- 2~4ì¤„ì˜ ê°„ê²°í•œ ë‹µë³€
- êµ¬ì²´ì  ì •ë³´ í•„ìˆ˜: ì¸ëª…, ë‚ ì§œ, ê¸°ê´€ëª…, ìˆ˜ì¹˜
- ì¸ìš©í•œ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ ê°ì£¼ í‘œì‹œ
- ê°ê´€ì  ì‚¬ì‹¤ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±

ë‹µë³€ ì‘ì„±:"""

        # AI ëª¨ë¸ ì§ì ‘ í˜¸ì¶œ
        ai_response = bedrock_runtime.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "messages": [
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ]
            })
        )
        
        result = json.loads(ai_response['body'].read())
        answer = result['content'][0]['text'].strip()
        
        # 4. ì‘ë‹µ êµ¬ì¡° ìƒì„± (retrieveAndGenerateì™€ í˜¸í™˜ë˜ë„ë¡)
        combined_response = {
            "output": {
                "text": answer
            },
            "citations": [{
                "generatedResponsePart": {
                    "textResponsePart": {
                        "span": {"start": 0, "end": len(answer)},
                        "text": answer
                    }
                },
                "retrievedReferences": []
            }],
            "sessionId": f"manual-{datetime.now().isoformat()}"
        }
        
        # retrieve ê²°ê³¼ë¥¼ citationsì— ì¶”ê°€
        for retrieval_result in retrieval_results:
            reference = {
                "content": {
                    "text": retrieval_result.get('content', {}).get('text', '')
                },
                "location": retrieval_result.get('location', {}),
                "metadata": retrieval_result.get('metadata', {})
            }
            combined_response['citations'][0]["retrievedReferences"].append(reference)
        
        logger.info("Successfully retrieved and generated response with manual AI call")
        logger.info(f"Generated answer: {answer[:200]}...")
        
        return combined_response
        
    except ClientError as e:
        error_code = e.response.get("Error", {}).get("Code", "Unknown")
        error_message = e.response.get("Error", {}).get("Message", "Unknown error")
        logger.error(f"Bedrock API error: {error_code} - {error_message}")
        raise ChatbotError(f"ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}")
    
    except Exception as e:
        logger.error(f"Unexpected error in retrieve_and_generate_with_references: {str(e)}")
        raise ChatbotError("ë‹µë³€ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


def query_perplexity(question: str, max_tokens: int = 512) -> str:
    """Fallback to Perplexity AI when Knowledge Base returns no result"""
    if not PERPLEXITY_API_KEY:
        raise ChatbotError("Perplexity API key not configured")

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json",
    }
    body = {
        "model": "pplx-70b-online",  # ìµœì‹  ì˜¨ë¼ì¸ ëª¨ë¸
        "messages": [{"role": "user", "content": question}],
        "max_tokens": max_tokens,
    }

    try:
        resp = requests.post(PPLX_URL, headers=headers, json=body, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        return (
            data.get("choices", [{}])[0]
            .get("message", {})
            .get("content", "ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            .strip()
        )
    except Exception as err:
        logger.error(f"Perplexity API error: {err}")
        raise ChatbotError("Perplexity API í˜¸ì¶œ ì‹¤íŒ¨")


def validate_request_body(body: Dict[str, Any]) -> str:
    """ìš”ì²­ ë³¸ë¬¸ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ê³  ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not isinstance(body, dict):
        raise ChatbotError("ìš”ì²­ ë³¸ë¬¸ì€ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    question = body.get("question", "").strip()
    if not question:
        raise ChatbotError("ì§ˆë¬¸(question) í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    if len(question) > 1000:
        raise ChatbotError("ì§ˆë¬¸ì€ 1000ìë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return question


def extract_metadata_from_s3(s3_uri: str) -> Dict[str, str]:
    """S3 URIì—ì„œ ì›ë³¸ .md íŒŒì¼ì„ ì½ì–´ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    metadata = {
        "title": "",
        "date": "",
        "author": "",
        "media": "ì„œìš¸ê²½ì œ",
        "url": ""
    }
    
    try:
        # S3 URI íŒŒì‹± (s3://bucket-name/path/to/file.md)
        parsed_uri = urlparse(s3_uri)
        bucket_name = parsed_uri.netloc
        object_key = parsed_uri.path.lstrip('/')
        
        logger.info(f"Reading S3 file: bucket={bucket_name}, key={object_key}")
        
        # S3ì—ì„œ íŒŒì¼ ì½ê¸°
        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
        content = response['Body'].read().decode('utf-8')
        
        # .md íŒŒì¼ì—ì„œ ê¸°ì‚¬ë“¤ì„ ë¶„ë¦¬ (--- êµ¬ë¶„ì ì‚¬ìš©)
        articles = content.split('\n---\n')
        
        # ì²« ë²ˆì§¸ ì‹¤ì œ ê¸°ì‚¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í—¤ë” ë¶€ë¶„ ì œì™¸)
        if len(articles) > 1:
            first_article = articles[1]  # ì²« ë²ˆì§¸ ê¸°ì‚¬
            
            # ì œëª© ì¶”ì¶œ - ### ìˆ«ì. ì œëª© íŒ¨í„´
            title_match = re.search(r'###\s*\d+\.\s*(.+?)(?:\n|$)', first_article)
            if title_match:
                metadata["title"] = title_match.group(1).strip()
            
            # ë°œí–‰ì¼ ì¶”ì¶œ
            date_match = re.search(r'\*\*ë°œí–‰ì¼:\*\*\s*([^\n]+)', first_article)
            if date_match:
                date_str = date_match.group(1).strip()
                # ISO ë‚ ì§œë¥¼ í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                try:
                    # 2016-04-10T00:00:00.000+09:00 í˜•ì‹ ì²˜ë¦¬
                    if 'T' in date_str:
                        dt = datetime.fromisoformat(date_str.replace('T00:00:00.000+09:00', ''))
                        metadata["date"] = dt.strftime('%Yë…„ %mì›” %dì¼')
                    else:
                        metadata["date"] = date_str
                except:
                    metadata["date"] = date_str
            
            # ê¸°ì ì¶”ì¶œ
            author_match = re.search(r'\*\*ê¸°ì:\*\*\s*([^\n]+)', first_article)
            if author_match:
                metadata["author"] = author_match.group(1).strip()
            
            # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
            media_match = re.search(r'\*\*ì–¸ë¡ ì‚¬:\*\*\s*([^\n]+)', first_article)
            if media_match:
                metadata["media"] = media_match.group(1).strip()
            
            # URL ì¶”ì¶œ
            url_match = re.search(r'\*\*URL:\*\*\s*([^\n\s]+)', first_article)
            if url_match:
                metadata["url"] = url_match.group(1).strip()
            
            logger.info(f"Extracted metadata from S3: {metadata}")
        
    except Exception as e:
        logger.warning(f"Failed to extract metadata from S3 {s3_uri}: {str(e)}")
    
    return metadata


def find_best_matching_article(s3_uri: str, query_chunk: str) -> Dict[str, str]:
    """S3 íŒŒì¼ì—ì„œ ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ë¥¼ ì°¾ì•„ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # S3 URI íŒŒì‹±
        parsed_uri = urlparse(s3_uri)
        bucket_name = parsed_uri.netloc
        object_key = parsed_uri.path.lstrip('/')
        
        # S3ì—ì„œ íŒŒì¼ ì½ê¸°
        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
        content = response['Body'].read().decode('utf-8')
        
        # .md íŒŒì¼ì—ì„œ ê¸°ì‚¬ë“¤ì„ ë¶„ë¦¬
        articles = content.split('\n---\n')
        
        best_metadata = None
        max_relevance = 0
        
        # ê° ê¸°ì‚¬ì—ì„œ ê´€ë ¨ì„± ê²€ì‚¬
        for i, article in enumerate(articles[1:], 1):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'###\s*\d+\.\s*(.+?)(?:\n|$)', article)
            title = title_match.group(1).strip() if title_match else ""
            
            # ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì¿¼ë¦¬ ì²­í¬ì™€ì˜ ê´€ë ¨ì„± ê³„ì‚°
            relevance = 0
            query_words = set(query_chunk.lower().split())
            article_words = set(article.lower().split())
            
            # ë‹¨ì–´ ë§¤ì¹­ìœ¼ë¡œ ê°„ë‹¨í•œ ê´€ë ¨ì„± ê³„ì‚°
            common_words = query_words.intersection(article_words)
            if query_words:
                relevance = len(common_words) / len(query_words)
            
            # ì œëª©ì— ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€
            if any(word in title.lower() for word in query_words):
                relevance += 0.3
            
            logger.info(f"Article {i} relevance: {relevance:.3f}, title: {title[:50]}...")
            
            if relevance > max_relevance:
                max_relevance = relevance
                
                # í•´ë‹¹ ê¸°ì‚¬ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = {
                    "title": title,
                    "date": "",
                    "author": "",
                    "media": "ì„œìš¸ê²½ì œ",
                    "url": ""
                }
                
                # ë°œí–‰ì¼ ì¶”ì¶œ
                date_match = re.search(r'\*\*ë°œí–‰ì¼:\*\*\s*([^\n]+)', article)
                if date_match:
                    date_str = date_match.group(1).strip()
                    try:
                        if 'T' in date_str:
                            dt = datetime.fromisoformat(date_str.replace('T00:00:00.000+09:00', ''))
                            metadata["date"] = dt.strftime('%Yë…„ %mì›” %dì¼')
                        else:
                            metadata["date"] = date_str
                    except:
                        metadata["date"] = date_str
                
                # ê¸°ì ì¶”ì¶œ
                author_match = re.search(r'\*\*ê¸°ì:\*\*\s*([^\n]+)', article)
                if author_match:
                    metadata["author"] = author_match.group(1).strip()
                
                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                media_match = re.search(r'\*\*ì–¸ë¡ ì‚¬:\*\*\s*([^\n]+)', article)
                if media_match:
                    metadata["media"] = media_match.group(1).strip()
                
                # URL ì¶”ì¶œ
                url_match = re.search(r'\*\*URL:\*\*\s*([^\n\s]+)', article)
                if url_match:
                    metadata["url"] = url_match.group(1).strip()
                
                best_metadata = metadata
        
        logger.info(f"Best matching article metadata: {best_metadata}")
        return best_metadata or extract_metadata_from_s3(s3_uri)
        
    except Exception as e:
        logger.warning(f"Failed to find best matching article: {str(e)}")
        return extract_metadata_from_s3(s3_uri)


def handle_chat(event: Dict[str, Any]) -> Dict[str, Any]:
    """ì±—ë´‡ ëŒ€í™” ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        # API Gateway ì´ë²¤íŠ¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
        if 'body' in event:
            if isinstance(event['body'], str):
                request_body = json.loads(event['body'])
            else:
                request_body = event['body']
        else:
            request_body = event
            
        question = validate_request_body(request_body)
        
        logger.info(f"Processing chat request: {question}")
        
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
        try:
            response = orchestrated_news_search(question)
            logger.info("Successfully used orchestrated search")
        except Exception as e:
            logger.warning(f"Orchestrated search failed: {e}, falling back to traditional approach")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            if is_typo(question):
                logger.info("Typo detected â€“ invoking Perplexity spellfix")
                try:
                    corrected_q, extra_ctx = perplexity_spellfix(question)
                except ChatbotError as ce:
                    logger.warning(f"Spellfix failed: {ce}")
                    corrected_q, extra_ctx = question, ""
                response = retrieve_and_generate_with_references(corrected_q, extra_context=extra_ctx)

            elif needs_external_search(question):
                logger.info("Date-related hard question â€“ invoking Perplexity refine")
                try:
                    refined_q, extra_ctx = perplexity_refine(question)
                except ChatbotError as ce:
                    logger.warning(f"Refine failed: {ce}")
                    refined_q, extra_ctx = question, ""
                response = retrieve_and_generate_with_references(refined_q, extra_context=extra_ctx)

            else:  # easy path
                response = retrieve_and_generate_with_references(question)
        
        answer = response.get("output", {}).get("text", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        citations = response.get("citations", [])
        
        logger.info(f"Retrieved {len(citations)} citations")
        
        # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (S3ì—ì„œ ì›ë³¸ íŒŒì¼ ì½ì–´ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)
        sources = []
        processed_locations = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        
        # ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§ì„ ìœ„í•œ target years ì¶”ì¶œ
        target_years = []
        current_year = datetime.now().year
        
        # ì§ˆë¬¸ì—ì„œ ë…„ë„ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„
        if any(keyword in question.lower() for keyword in ["2025ë…„", "ì˜¬í•´", "ìµœê·¼", "í˜„ì¬", "ì§€ê¸ˆ"]):
            target_years = [str(current_year)]
        elif any(keyword in question.lower() for keyword in ["2024ë…„", "ì‘ë…„", "ì§€ë‚œí•´"]):
            target_years = [str(current_year-1)]
        elif "2023ë…„" in question.lower():
            target_years = ["2023"]
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ìµœê·¼ 2ë…„ ë°ì´í„° í—ˆìš©
            target_years = [str(current_year), str(current_year-1)]
        
        logger.info(f"Target years for filtering: {target_years} (based on question: '{question}')")
        
        logger.info(f"=== DEBUG: Full Bedrock response structure ===")
        logger.info(f"Citations count: {len(citations)}")
        
        for i, citation in enumerate(citations):
            logger.info(f"=== Processing citation {i} ===")
            logger.info(f"Citation structure: {json.dumps(citation, default=str, ensure_ascii=False)}")
            
            retrieved_refs = citation.get("retrievedReferences", [])
            logger.info(f"Retrieved references count: {len(retrieved_refs)}")
            
            for j, reference in enumerate(retrieved_refs):
                logger.info(f"=== Processing reference {j} ===")
                logger.info(f"Reference structure: {json.dumps(reference, default=str, ensure_ascii=False)}")
                
                content = reference.get("content", {}).get("text", "")
                location = reference.get("location", {})
                
                logger.info(f"Content length: {len(content)}")
                logger.info(f"Location structure: {json.dumps(location, default=str, ensure_ascii=False)}")
                
                # S3 locationì—ì„œ URI ì¶”ì¶œ
                s3_location = location.get("s3Location", {})
                s3_uri = s3_location.get("uri", "")
                
                logger.info(f"Extracted S3 URI: '{s3_uri}'")
                
                if s3_uri and s3_uri not in processed_locations:
                    processed_locations.add(s3_uri)
                    
                    # S3ì—ì„œ ì›ë³¸ íŒŒì¼ì„ ì½ì–´ ìµœì ì˜ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    try:
                        metadata = find_best_matching_article(s3_uri, content)
                        logger.info(f"Metadata extraction result: {metadata}")
                        
                        if metadata and metadata.get("title"):
                            # ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§ ì ìš©
                            article_date = metadata.get("date", "")
                            date_match = False
                            
                            # target_yearsì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë§Œ í¬í•¨
                            if target_years:
                                date_match = any(year in article_date for year in target_years)
                                logger.info(f"Date filtering: '{article_date}' matches target years {target_years}: {date_match}")
                            else:
                                date_match = True  # target_yearsê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê¸°ì‚¬ í—ˆìš©
                            
                            if date_match:
                                source_info = {
                                    "title": metadata["title"],
                                    "date": metadata["date"] or "ë‚ ì§œ ì—†ìŒ", 
                                    "author": metadata["author"],
                                    "media": metadata["media"],
                                    "url": metadata["url"],
                                    "s3_uri": s3_uri
                                }
                                sources.append(source_info)
                                logger.info(f"âœ… Successfully added source (date matched): {source_info}")
                            else:
                                logger.info(f"ğŸš« Filtered out source due to date mismatch: {metadata['title']} ({article_date})")
                        else:
                            logger.warning(f"âŒ No valid metadata extracted from {s3_uri}")
                    except Exception as e:
                        logger.error(f"âŒ Error extracting metadata from {s3_uri}: {str(e)}")
                else:
                    if not s3_uri:
                        logger.warning(f"âŒ Empty S3 URI in reference {j}")
                        logger.info(f"Raw location data: {location}")
                    else:
                        logger.info(f"ğŸ”„ S3 URI already processed: {s3_uri}")
        
        logger.info(f"=== FINAL SOURCES COUNT: {len(sources)} ===")
        for idx, source in enumerate(sources):
            logger.info(f"Source {idx}: {source}")
        
        # ë‚ ì§œ í•„í„°ë§ í›„ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€
        if len(sources) < 2 and target_years:
            logger.warning(f"Very few sources ({len(sources)}) after date filtering for years {target_years}")
            # ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¥í•˜ì—¬ ì¬ê²€ìƒ‰ ê¶Œìœ  ë©”ì‹œì§€ ì¶”ê°€
            if len(sources) == 0:
                logger.warning("No sources found matching date criteria, falling back to all available sources")
                # ë‚ ì§œ í•„í„°ë§ì„ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ ì¬ê²€ìƒ‰
                sources = []
                processed_locations = set()
                
                for i, citation in enumerate(citations):
                    retrieved_refs = citation.get("retrievedReferences", [])
                    for j, reference in enumerate(retrieved_refs):
                        content = reference.get("content", {}).get("text", "")
                        location = reference.get("location", {})
                        s3_location = location.get("s3Location", {})
                        s3_uri = s3_location.get("uri", "")
                        
                        if s3_uri and s3_uri not in processed_locations:
                            processed_locations.add(s3_uri)
                            try:
                                metadata = find_best_matching_article(s3_uri, content)
                                if metadata and metadata.get("title"):
                                    source_info = {
                                        "title": metadata["title"],
                                        "date": metadata["date"] or "ë‚ ì§œ ì—†ìŒ", 
                                        "author": metadata["author"],
                                        "media": metadata["media"],
                                        "url": metadata["url"],
                                        "s3_uri": s3_uri
                                    }
                                    sources.append(source_info)
                                    logger.info(f"âœ… Fallback: Added source without date filter: {source_info}")
                                    if len(sources) >= 3:  # ìµœì†Œ 3ê°œ í™•ë³´í•˜ë©´ ì¤‘ë‹¨
                                        break
                            except Exception as e:
                                logger.error(f"âŒ Fallback error: {str(e)}")
                    if len(sources) >= 3:
                        break
        
        # ìµœëŒ€ 5ê°œ ì¶œì²˜ë§Œ ì‚¬ìš© (ê°ì£¼ì™€ ì¼ì¹˜)
        top_sources = sources[:5]
        
        # AIê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ê°ì£¼ë¥¼ ìƒì„±í–ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        footnoted_answer = answer
        
        # Perplexity ì‚¬ìš© ì—¬ë¶€ í‘œì‹œ
        used_perplexity = bool(PERPLEXITY_API_KEY and (is_typo(question) or needs_external_search(question)))
        
        result = {
            "answer": footnoted_answer,
            "sources": top_sources,
            "question": question,
            "timestamp": response.get("sessionId", ""),
            "enhanced_search": used_perplexity
        }
        
        logger.info(f"Generated response with {len(top_sources)} sources")
        
        # API Gateway ì‘ë‹µ í˜•ì‹
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization"
            },
            "body": json.dumps(result, ensure_ascii=False)
        }
        
    except ChatbotError as e:
        logger.warning(f"Chatbot error: {str(e)} â€“ trying Perplexity fallback")
        try:
            fallback_answer = query_perplexity(question)
            result = {
                "answer": fallback_answer,
                "sources": [],  # Perplexityì—ì„œ ë³„ë„ ì¶œì²˜ ì œê³µí•˜ì§€ ì•ŠìŒ
                "question": question,
                "timestamp": datetime.utcnow().isoformat()
            }
            return {
                "statusCode": 200,
                "headers": {
                    "Content-Type": "application/json",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Methods": "POST, OPTIONS",
                    "Access-Control-Allow-Headers": "Content-Type, Authorization",
                },
                "body": json.dumps(result, ensure_ascii=False),
            }
        except ChatbotError as pe:
            # Perplexityë„ ì‹¤íŒ¨ ì‹œ ì›ë˜ 400 ì‘ë‹µ
            return {
                "statusCode": 400,
                "headers": {
                    "Content-Type": "application/json",
                    "Access-Control-Allow-Origin": "*"
                },
                "body": json.dumps({
                    "error": str(pe),
                    "type": "validation_error"
                }, ensure_ascii=False)
            }
        
    except Exception as e:
        logger.error(f"Unexpected error in handle_chat: {str(e)}")
        return {
            "statusCode": 500,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps({
                "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "type": "internal_error"
            }, ensure_ascii=False)
        }


def health_check(event: Dict[str, Any]) -> Dict[str, Any]:
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        if not KNOWLEDGE_BASE_ID:
            return {
                "statusCode": 500,
                "headers": {
                    "Content-Type": "application/json",
                    "Access-Control-Allow-Origin": "*"
                },
                "body": json.dumps({
                    "status": "unhealthy",
                    "message": "Knowledge Base IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                }, ensure_ascii=False)
            }
        
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps({
                "status": "healthy",
                "service": "news-chatbot-simple",
                "knowledge_base_id": KNOWLEDGE_BASE_ID,
                "version": "1.0.0"
            }, ensure_ascii=False)
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            "statusCode": 500,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps({
                "status": "unhealthy",
                "error": str(e)
            }, ensure_ascii=False)
        }


def lambda_handler(event: Dict[str, Any], context) -> Dict[str, Any]:
    """Lambda í•¨ìˆ˜ì˜ ë©”ì¸ í•¸ë“¤ëŸ¬"""
    logger.info(f"Processing request: {json.dumps(event, default=str)}")
    
    try:
        # HTTP ë©”ì„œë“œì™€ ê²½ë¡œì— ë”°ë¼ ë¼ìš°íŒ…
        http_method = event.get("httpMethod", "POST")
        path = event.get("path", "/chat")
        
        if http_method == "OPTIONS":
            # CORS preflight ìš”ì²­ ì²˜ë¦¬
            return {
                "statusCode": 200,
                "headers": {
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
                    "Access-Control-Allow-Headers": "Content-Type, Authorization"
                },
                "body": ""
            }
        elif path == "/health" or path.endswith("/health"):
            return health_check(event)
        elif path == "/chat" or path.endswith("/chat"):
            return handle_chat(event)
        else:
            return {
                "statusCode": 404,
                "headers": {
                    "Content-Type": "application/json",
                    "Access-Control-Allow-Origin": "*"
                },
                "body": json.dumps({
                    "error": "Not Found",
                    "message": f"Path {path} not found"
                }, ensure_ascii=False)
            }
            
    except Exception as e:
        logger.error(f"Unhandled error in lambda handler: {str(e)}")
        return {
            "statusCode": 500,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps({
                "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "type": "internal_error"
            }, ensure_ascii=False)
        }


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
handler = lambda_handler