#!/usr/bin/env python3
"""
BigKinds ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
- JSON â†’ Markdown ë³€í™˜
- Knowledge Base í˜•ì‹ìœ¼ë¡œ ìë™ ë³€í™˜
- S3 ì—…ë¡œë“œ ì¤€ë¹„
"""

import os
import json
import requests
from datetime import datetime
from dateutil import parser, relativedelta
from dotenv import load_dotenv
from typing import Dict, List, Any
import argparse
import time

load_dotenv()

# BigKinds API ì„¤ì •
API_URL = "https://tools.kinds.or.kr/search/news"
ACCESS_KEY = os.getenv("BIGKINDS_KEY")

CATEGORIES = {
    "ì •ì¹˜": "001000000",
    "ê²½ì œ": "002000000",
    "ì‚¬íšŒ": "003000000",
    "ë¬¸í™”": "004000000",
    "êµ­ì œ": "005000000",
    "ì§€ì—­": "006000000",
    "ìŠ¤í¬ì¸ ": "007000000",
    "IT_ê³¼í•™": "008000000",
}

FIELDS = [
    "news_id", "title", "content", "byline", "publisher_name",
    "published_at", "provider_link_page",
    "category", "category_code", "year", "month"
]

def fetch_all_news(category_code: str, date_from: datetime, date_to: datetime) -> List[Dict]:
    """ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ë²”ìœ„ì˜ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_docs = []
    offset = 0
    size = 10000
    
    while True:
        payload = {
            "access_key": ACCESS_KEY,
            "argument": {
                "query": "",
                "published_at": {
                    "from": date_from.strftime("%Y-%m-%dT00:00:00"),
                    "until": date_to.strftime("%Y-%m-%dT23:59:59")
                },
                "category": [category_code],
                "sort": {"date": "asc"},
                "return_from": offset,
                "return_size": size,
                "fields": FIELDS
            }
        }
        
        try:
            r = requests.post(API_URL, json=payload, timeout=30)
            r.raise_for_status()
            ret = r.json()["return_object"]
            batch = ret["documents"]
            
            if not batch:
                break
                
            all_docs.extend(batch)
            offset += len(batch)
            
            if offset >= ret["total_hits"]:
                break
                
            # API ì†ë„ ì œí•œ ëŒ€ì‘
            time.sleep(0.5)
            
        except Exception as e:
            print(f"API ì˜¤ë¥˜: {e}")
            break
    
    # URL í•„ë“œ ì •ê·œí™”
    for doc in all_docs:
        doc["url"] = doc.pop("provider_link_page", None)
    
    return all_docs

def convert_to_markdown(articles: List[Dict], category: str, date: str) -> str:
    """JSON ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    md_content = f"# {date} {category} ë‰´ìŠ¤\n\n"
    md_content += f"**ìˆ˜ì§‘ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    md_content += f"**ì´ ê¸°ì‚¬ ìˆ˜**: {len(articles)}ê°œ\n\n"
    md_content += "---\n\n"
    
    for idx, article in enumerate(articles, 1):
        # ê¸°ì‚¬ ì œëª©
        title = article.get("title", "ì œëª© ì—†ìŒ")
        md_content += f"### {idx}. {title}\n\n"
        
        # ë©”íƒ€ë°ì´í„°
        md_content += f"**ë°œí–‰ì¼**: {article.get('published_at', 'N/A')}\n"
        md_content += f"**ì–¸ë¡ ì‚¬**: {article.get('publisher_name', 'N/A')}\n"
        md_content += f"**ê¸°ì**: {article.get('byline', 'N/A')}\n"
        md_content += f"**URL**: {article.get('url', 'N/A')}\n"
        md_content += f"**ì¹´í…Œê³ ë¦¬**: {category}\n\n"
        
        # ë³¸ë¬¸
        content = article.get("content", "ë‚´ìš© ì—†ìŒ")
        # ê¸´ ì¤„ ì²˜ë¦¬ (ë§ˆí¬ë‹¤ìš´ ê°€ë…ì„±)
        content = content.replace("\n", "\n\n")
        md_content += f"**ë‚´ìš©**:\n{content}\n\n"
        
        md_content += "---\n\n"
    
    return md_content

def save_as_jsonl_for_knowledge_base(articles: List[Dict], output_path: str, category: str, date: str):
    """Knowledge Baseìš© JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, 'w', encoding='utf-8') as f:
        for article in articles:
            # Knowledge Baseì— í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            kb_doc = {
                "chunk": article.get("content", ""),
                "title": article.get("title", ""),
                "date": date,
                "url": article.get("url", ""),
                "category": category,
                "publisher": article.get("publisher_name", ""),
                "byline": article.get("byline", ""),
                "article_id": article.get("news_id", ""),
                "metadata": {
                    "source": "BigKinds",
                    "collection_date": datetime.now().isoformat()
                }
            }
            f.write(json.dumps(kb_doc, ensure_ascii=False) + '\n')

def main():
    parser = argparse.ArgumentParser(description="BigKinds ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë³€í™˜")
    parser.add_argument("--start-date", default="2023-02-17", help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", default="2025-07-20", help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--output-format", choices=["json", "markdown", "jsonl", "all"], default="all", 
                       help="ì¶œë ¥ í˜•ì‹ ì„ íƒ")
    parser.add_argument("--output-dir", default="output", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    args = parser.parse_args()
    
    START_DATE = parser.parse(args.start_date)
    END_DATE = parser.parse(args.end_date)
    
    current = START_DATE
    while current <= END_DATE:
        day_start = current
        day_end = current
        y, m, d = day_start.year, day_start.month, day_start.day
        date_str = f"{y:04d}-{m:02d}-{d:02d}"
        
        for name, code in CATEGORIES.items():
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ìˆ˜ì§‘ ì¤‘: {date_str} / {name}")
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            docs = fetch_all_news(code, day_start, day_end)
            
            if not docs:
                print(f"  â†’ ê¸°ì‚¬ ì—†ìŒ")
                continue
                
            print(f"  â†’ {len(docs)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì¶œë ¥ í´ë” ìƒì„±
            folder = os.path.join(args.output_dir, f"{y:04d}", f"{m:02d}", f"{d:02d}")
            os.makedirs(folder, exist_ok=True)
            
            # JSON ì €ì¥
            if args.output_format in ["json", "all"]:
                json_path = os.path.join(folder, f"{name}.json")
                meta = {
                    "year": y, "month": m, "day": d,
                    "category": name,
                    "total_articles": len(docs),
                    "collection_date": datetime.now().isoformat(),
                    "date_range": {
                        "from": day_start.strftime("%Y-%m-%d"),
                        "until": day_end.strftime("%Y-%m-%d")
                    }
                }
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({"metadata": meta, "articles": docs}, f, ensure_ascii=False, indent=2)
                print(f"  â†’ JSON ì €ì¥: {json_path}")
            
            # Markdown ì €ì¥
            if args.output_format in ["markdown", "all"]:
                md_path = os.path.join(folder, f"{name}.md")
                md_content = convert_to_markdown(docs, name, date_str)
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(md_content)
                print(f"  â†’ Markdown ì €ì¥: {md_path}")
            
            # JSONL ì €ì¥ (Knowledge Baseìš©)
            if args.output_format in ["jsonl", "all"]:
                jsonl_path = os.path.join(folder, f"{name}.jsonl")
                save_as_jsonl_for_knowledge_base(docs, jsonl_path, name, date_str)
                print(f"  â†’ JSONL ì €ì¥: {jsonl_path}")
        
        current += relativedelta.relativedelta(days=1)
    
    print("\nâœ… ë°ì´í„° ìˆ˜ì§‘ ë° ë³€í™˜ ì™„ë£Œ!")
    
    # S3 ì—…ë¡œë“œ ëª…ë ¹ì–´ ì¶œë ¥
    print("\nğŸ“¤ S3 ì—…ë¡œë“œ ëª…ë ¹ì–´:")
    print(f"aws s3 sync {args.output_dir}/ s3://seoul-economic-news-data-2025/bigkinds-data/ --exclude '*.json'")
    
    # Knowledge Base ì •ë³´
    print("\nğŸ—„ï¸ Knowledge Base ì •ë³´:")
    print("- Knowledge Base ID: PGQV3JXPET")
    print("- Data Source ID: W8DS8YQGZG")
    print("- S3 ë²„í‚·: seoul-economic-news-data-2025")
    print("\në™ê¸°í™” ëª…ë ¹ì–´:")
    print("aws bedrock-agent start-ingestion-job --knowledge-base-id PGQV3JXPET --data-source-id W8DS8YQGZG")

if __name__ == "__main__":
    main()